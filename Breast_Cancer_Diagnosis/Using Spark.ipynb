{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# **K-Means using Spark**"
      ],
      "metadata": {
        "id": "GzuRMgIu3jnA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Setup\n",
        "\n",
        "`PySpark` is the `Python API` for `Apache Spark`, an open source, distributed computing framework and set of libraries for real-time, large-scale data processing. \n",
        "\n",
        "Let's setup Spark on your Colab envrironment."
      ],
      "metadata": {
        "id": "OGhamjSc3qeB"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "K4uW6IM93cY1",
        "outputId": "f1ccb923-cf73-4823-db4f-21e7a22656b4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting pyspark\n",
            "  Downloading pyspark-3.3.0.tar.gz (281.3 MB)\n",
            "\u001b[K     |████████████████████████████████| 281.3 MB 38 kB/s \n",
            "\u001b[?25hCollecting py4j==0.10.9.5\n",
            "  Downloading py4j-0.10.9.5-py2.py3-none-any.whl (199 kB)\n",
            "\u001b[K     |████████████████████████████████| 199 kB 51.2 MB/s \n",
            "\u001b[?25hBuilding wheels for collected packages: pyspark\n",
            "  Building wheel for pyspark (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pyspark: filename=pyspark-3.3.0-py2.py3-none-any.whl size=281764026 sha256=7b5ef7216e52a3f0747dc6e2d1e93c412b4c47fa414b2035d3324e5158337b6c\n",
            "  Stored in directory: /root/.cache/pip/wheels/7a/8e/1b/f73a52650d2e5f337708d9f6a1750d451a7349a867f928b885\n",
            "Successfully built pyspark\n",
            "Installing collected packages: py4j, pyspark\n",
            "Successfully installed py4j-0.10.9.5 pyspark-3.3.0\n"
          ]
        }
      ],
      "source": [
        "!pip install pyspark"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "PyDrive is a high-level Python wrapper for the ```Google Drive API```. It allows you to easily upload, download, and delete files in your Google Drive from a Python script.\n",
        "\n",
        "PyDrive is a simple yet powerful way for integrating Google Drive into your applications. It opens a lot of doors for all sorts of cool projects."
      ],
      "metadata": {
        "id": "ZQw2SIyTmENc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -U -q PyDrive \n",
        "# -q, --quiet option reduces the output produced by pip. It does not affect the installation process. It is a general option.\n",
        " # -q means display only the messages with WARNING,ERROR,CRITICAL log levels\n",
        "# -U, --upgrade Upgrade all packages to the newest available version."
      ],
      "metadata": {
        "id": "Vy7eyVjq3lOF"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**OpenJDK Development Kit (JDK) (headless)** is a run time environment\n",
        "\n",
        "The headless option is a minimal runtime environment, without a graphical interface, more suitable for server applications. It uses minimal system resources and doesn’t include keyboard or mouse support.\n",
        "\n",
        "OpenJDK is a development environment for building applications, applets, and components using the Java programming language.\n",
        "\n",
        "Install **OpenJDK 11 headless** by entering the following:"
      ],
      "metadata": {
        "id": "CVeockAzmKad"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!apt install openjdk-8-jdk-headless -qq\n",
        "# apt package management system is distinguished by excellent support for dependency management and tracking. This means that apt will try to resolve dependencies, searching for the needed packages amongst all of the repositories it has been told about.\n",
        "# -qq  means display only the messages with ERROR,CRITICAL log levels"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c7h9uaSC3zmW",
        "outputId": "95ae278a-1ec2-46f6-a918-1bbdd85dafdf"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The following package was automatically installed and is no longer required:\n",
            "  libnvidia-common-460\n",
            "Use 'apt autoremove' to remove it.\n",
            "The following additional packages will be installed:\n",
            "  openjdk-8-jre-headless\n",
            "Suggested packages:\n",
            "  openjdk-8-demo openjdk-8-source libnss-mdns fonts-dejavu-extra\n",
            "  fonts-ipafont-gothic fonts-ipafont-mincho fonts-wqy-microhei\n",
            "  fonts-wqy-zenhei fonts-indic\n",
            "The following NEW packages will be installed:\n",
            "  openjdk-8-jdk-headless openjdk-8-jre-headless\n",
            "0 upgraded, 2 newly installed, 0 to remove and 27 not upgraded.\n",
            "Need to get 36.6 MB of archives.\n",
            "After this operation, 143 MB of additional disk space will be used.\n",
            "Selecting previously unselected package openjdk-8-jre-headless:amd64.\n",
            "(Reading database ... 123942 files and directories currently installed.)\n",
            "Preparing to unpack .../openjdk-8-jre-headless_8u342-b07-0ubuntu1~18.04_amd64.deb ...\n",
            "Unpacking openjdk-8-jre-headless:amd64 (8u342-b07-0ubuntu1~18.04) ...\n",
            "Selecting previously unselected package openjdk-8-jdk-headless:amd64.\n",
            "Preparing to unpack .../openjdk-8-jdk-headless_8u342-b07-0ubuntu1~18.04_amd64.deb ...\n",
            "Unpacking openjdk-8-jdk-headless:amd64 (8u342-b07-0ubuntu1~18.04) ...\n",
            "Setting up openjdk-8-jre-headless:amd64 (8u342-b07-0ubuntu1~18.04) ...\n",
            "update-alternatives: using /usr/lib/jvm/java-8-openjdk-amd64/jre/bin/orbd to provide /usr/bin/orbd (orbd) in auto mode\n",
            "update-alternatives: using /usr/lib/jvm/java-8-openjdk-amd64/jre/bin/servertool to provide /usr/bin/servertool (servertool) in auto mode\n",
            "update-alternatives: using /usr/lib/jvm/java-8-openjdk-amd64/jre/bin/tnameserv to provide /usr/bin/tnameserv (tnameserv) in auto mode\n",
            "Setting up openjdk-8-jdk-headless:amd64 (8u342-b07-0ubuntu1~18.04) ...\n",
            "update-alternatives: using /usr/lib/jvm/java-8-openjdk-amd64/bin/idlj to provide /usr/bin/idlj (idlj) in auto mode\n",
            "update-alternatives: using /usr/lib/jvm/java-8-openjdk-amd64/bin/wsimport to provide /usr/bin/wsimport (wsimport) in auto mode\n",
            "update-alternatives: using /usr/lib/jvm/java-8-openjdk-amd64/bin/jsadebugd to provide /usr/bin/jsadebugd (jsadebugd) in auto mode\n",
            "update-alternatives: using /usr/lib/jvm/java-8-openjdk-amd64/bin/native2ascii to provide /usr/bin/native2ascii (native2ascii) in auto mode\n",
            "update-alternatives: using /usr/lib/jvm/java-8-openjdk-amd64/bin/javah to provide /usr/bin/javah (javah) in auto mode\n",
            "update-alternatives: using /usr/lib/jvm/java-8-openjdk-amd64/bin/clhsdb to provide /usr/bin/clhsdb (clhsdb) in auto mode\n",
            "update-alternatives: using /usr/lib/jvm/java-8-openjdk-amd64/bin/xjc to provide /usr/bin/xjc (xjc) in auto mode\n",
            "update-alternatives: using /usr/lib/jvm/java-8-openjdk-amd64/bin/hsdb to provide /usr/bin/hsdb (hsdb) in auto mode\n",
            "update-alternatives: using /usr/lib/jvm/java-8-openjdk-amd64/bin/schemagen to provide /usr/bin/schemagen (schemagen) in auto mode\n",
            "update-alternatives: using /usr/lib/jvm/java-8-openjdk-amd64/bin/extcheck to provide /usr/bin/extcheck (extcheck) in auto mode\n",
            "update-alternatives: using /usr/lib/jvm/java-8-openjdk-amd64/bin/jhat to provide /usr/bin/jhat (jhat) in auto mode\n",
            "update-alternatives: using /usr/lib/jvm/java-8-openjdk-amd64/bin/wsgen to provide /usr/bin/wsgen (wsgen) in auto mode\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The **OS module** in Python provides functions for interacting with the operating system. This module provides a portable way of using operating system-dependent functionality.\n",
        "\n",
        "The *os* module include many functions to interact with the file system."
      ],
      "metadata": {
        "id": "0Ur2Gw73mPuc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**os.environ** in Python is a mapping object that represents the user’s environmental variables. It returns a dictionary having user’s environmental variable as key and their values as value.\n",
        "\n",
        "        Syntax: os.environ\n",
        "\n",
        "        Parameter: It is a non-callable object. Hence, no parameter is required\n",
        "\n",
        "        Return Type: This returns a dictionary representing the user’s environmental variables"
      ],
      "metadata": {
        "id": "w4GtygS8mSid"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**JAVA_HOME** is an operating system (OS) environment variable which can optionally be set after either the *`Java Development Kit (JDK)`* or the *`Java Runtime Environment (JRE)`* is installed. The `JAVA_HOME` environment variable points to the file system location where the `JDK` or `JRE` was installed.\n",
        "\n",
        "Other programs installed on a desktop computer that require a Java runtime will query the OS for the **JAVA_HOME** variable to find out where the runtime is installed"
      ],
      "metadata": {
        "id": "1ii8toIZmVGI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "os.environ[\"JAVA_HOME\"] = '/usr/lib/jvm/java-8-openjdk-amd64'"
      ],
      "metadata": {
        "id": "TopXsllB34VZ"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# importing necessary libraries\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "\n",
        "import pyspark # importing pyspark after installed\n",
        "# import * as opposed to other forms of import is that it imports everything from the designated module under the current module.\n",
        "from pyspark.sql import * # PySpark SQL is one of the most used PySpark modules which is used for processing structured columnar data format.\n",
        "from pyspark.sql.types import * # importing all(*) list of data types available using pyspark.sql.types\n",
        "from pyspark.sql.functions import * # It represents a list of built-in functions available for DataFrame.\n",
        "from pyspark import SparkContext, SparkConf # 'SparkContext' is the entry point to any functionality.\n",
        "                                            # 'SparkConf' represents a list of built-in functions available for DataFrame."
      ],
      "metadata": {
        "id": "L8w5_g9439Y6"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**SparkContext** is the entry point to any spark functionality. When we run any Spark application, a driver program starts, which has the main function and your SparkContext gets initiated here. The driver program then runs the operations inside the executors on worker nodes.\n",
        "\n",
        "A **SparkContext** represents the connection to a Spark cluster, and can be used to create RDDs, accumulators and broadcast variables on that cluster.\n",
        "\n",
        "Only one **SparkContext** should be active per **JVM**. You must `stop()` the active `SparkContext` before creating a new one."
      ],
      "metadata": {
        "id": "Ssxmcc1kmZbj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**SparkConf** creates Configuration for a Spark application. Used to set various Spark parameters as key-value pairs.\n",
        "\n",
        "`SparkConf()`, loads values from spark.* Java system properties as well. In this case, any parameters you set directly on the SparkConf object take priority over system properties."
      ],
      "metadata": {
        "id": "UBcpF-MFrZWi"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "    Every SparkContext launches a 'Web UI', by \"default on port 4040\", that displays useful information about the application. This includes:\n",
        "\n",
        "        A list of scheduler stages and tasks\n",
        "        A summary of RDD sizes and memory usage\n",
        "        Environmental information.\n",
        "        Information about the running executors\n",
        "\n",
        "    If multiple SparkContexts are running on the same host, they will bind to successive ports beginning with 4040 (4041, 4042, etc)."
      ],
      "metadata": {
        "id": "mk2gAIBj6xAo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# create the session\n",
        "conf = SparkConf().set(\"spark.ui.port\",\"4050\") # configured spark application by setting Web UI on port \"4050\"\n",
        "\n",
        "# create the context\n",
        "sc = pyspark.SparkContext(conf = conf) # creating an entry point to previously created spark application, 'conf'.\n",
        "spark = SparkSession.builder.getOrCreate() # Gets an existing SparkSession or, if there is no existing one, creates a new one based on the options set in this builder."
      ],
      "metadata": {
        "id": "bdESOUFd6vEH"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "You can easily check the current version and get the link of the web interface. In the Spark UI, you can monitor the progress of your job and debug the performance bottlenecks (if you Colab is running with a **local runtime**)."
      ],
      "metadata": {
        "id": "kZs_8-qC7pAj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "spark"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 219
        },
        "id": "2IispaJ37lc3",
        "outputId": "eae2aa89-9600-4f1d-8279-d0bf8d36cdd1"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<pyspark.sql.session.SparkSession at 0x7f7c2fb93d50>"
            ],
            "text/html": [
              "\n",
              "            <div>\n",
              "                <p><b>SparkSession - in-memory</b></p>\n",
              "                \n",
              "        <div>\n",
              "            <p><b>SparkContext</b></p>\n",
              "\n",
              "            <p><a href=\"http://4276e058aa41:4050\">Spark UI</a></p>\n",
              "\n",
              "            <dl>\n",
              "              <dt>Version</dt>\n",
              "                <dd><code>v3.3.0</code></dd>\n",
              "              <dt>Master</dt>\n",
              "                <dd><code>local[*]</code></dd>\n",
              "              <dt>AppName</dt>\n",
              "                <dd><code>pyspark-shell</code></dd>\n",
              "            </dl>\n",
              "        </div>\n",
              "        \n",
              "            </div>\n",
              "        "
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 'wget' is a networking command-line tool that lets you download files and interact with REST APIs.\n",
        "!wget https://bin.equinox.io/c/4VmDzA7iaHb/ngrok-stable-linux-amd64.zip # ngrok is the fastest way to put anything on the internet, getting zip file"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "i3Z1xoTr-9hM",
        "outputId": "51e85d27-ddc9-4f59-c19d-9ef9be81c0bc"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2022-10-20 19:10:21--  https://bin.equinox.io/c/4VmDzA7iaHb/ngrok-stable-linux-amd64.zip\n",
            "Resolving bin.equinox.io (bin.equinox.io)... 54.161.241.46, 52.202.168.65, 54.237.133.81, ...\n",
            "Connecting to bin.equinox.io (bin.equinox.io)|54.161.241.46|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 13832437 (13M) [application/octet-stream]\n",
            "Saving to: ‘ngrok-stable-linux-amd64.zip.1’\n",
            "\n",
            "ngrok-stable-linux- 100%[===================>]  13.19M  60.5MB/s    in 0.2s    \n",
            "\n",
            "2022-10-20 19:10:22 (60.5 MB/s) - ‘ngrok-stable-linux-amd64.zip.1’ saved [13832437/13832437]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!unzip ngrok-stable-linux-amd64.zip # Unzipping the ngrok \n",
        "get_ipython().system_raw('./ngrok http 4050 &') # get_ipython() - Get the global InteractiveShell instance\n",
        "# The get_ipython() command allows one to access IPython commands, and system_raw() executes commands in the native command prompt / terminal.\n",
        "# system_raw - starts the ngrok tunnel via the http protocol to port 4050"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "73ONHhs8zc0W",
        "outputId": "238cc015-9aef-49d7-a5d2-cfeaf1ee90df"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Archive:  ngrok-stable-linux-amd64.zip\n",
            "replace ngrok? [y]es, [n]o, [A]ll, [N]one, [r]ename: y\n",
            "  inflating: ngrok                   \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!curl -s http://localhost:4040/api/tunnels | python3 -c \\\"import sys, json; print(json.load(sys.stdin)['tunnels'][0]['public_url'])\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lYf6t5FaziCR",
        "outputId": "cbc8894a-6c9c-4e45-9ca3-cc1a3a4fd78c"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/bin/bash: -c: line 0: syntax error near unexpected token `json.load'\n",
            "/bin/bash: -c: line 0: `curl -s http://localhost:4040/api/tunnels | python3 -c \\\"import sys, json; print(json.load(sys.stdin)['tunnels'][0]['public_url'])\"'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The curl command in Linux is used to run http requests. In this case, a request is being made to “http://localhost:4040/api/tunnels”. \n",
        "This is an `ngrok API` running locally that contains information about the tunnels that are operating.\n",
        "\n",
        "The information received from that curl http request is then sent to the local Python 3 application via the Linux pipe “|” operator. The results come into Python via sys.stdin in json format – and the public URL of the tunnel that has been created is printed to screen. "
      ],
      "metadata": {
        "id": "XfpWeMgyz87D"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Data Pre-processing**\n",
        "\n",
        "In this colab, rather than downloading a file from Google Drive, we will load a famous machine learning dataset, the Breast Cancer Wisconsin dataset, using the scikit-learn datasets loader."
      ],
      "metadata": {
        "id": "8UZ-o4xN9-YM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import load_breast_cancer # importing python in-built dataset from scikit-learn\n",
        "breast_cancer = load_breast_cancer() # loading imported data"
      ],
      "metadata": {
        "id": "g16dciGy9W55"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "        * Creates a DataFrame from an RDD, a list or a pandas.DataFrame.\n",
        "          \n",
        "            # SparkSession.createDataFrame(data, schema=None, samplingRatio=None, verifySchema=True)[source]\n",
        "        \n",
        "        * Using default schema=None, it will try to infer the schema (column names and types) from data, \n",
        "        which should be an RDD of either Row, namedtuple, or dict.\n",
        "        \n",
        "        * If schema inference is needed, samplingRatio is used to determined the ratio of rows used for schema inference. \n",
        "        The first row will be used if samplingRatio is None."
      ],
      "metadata": {
        "id": "99-Emft_2Fnp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pd_df  = pd.DataFrame(breast_cancer.data, columns = breast_cancer.feature_names) # creating data frame of independent features\n",
        "df = spark.createDataFrame(pd_df) \n",
        "df"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nifHF0gy0xDZ",
        "outputId": "400b6ef2-915e-40db-8538-8316ccac1f9f"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "DataFrame[mean radius: double, mean texture: double, mean perimeter: double, mean area: double, mean smoothness: double, mean compactness: double, mean concavity: double, mean concave points: double, mean symmetry: double, mean fractal dimension: double, radius error: double, texture error: double, perimeter error: double, area error: double, smoothness error: double, compactness error: double, concavity error: double, concave points error: double, symmetry error: double, fractal dimension error: double, worst radius: double, worst texture: double, worst perimeter: double, worst area: double, worst smoothness: double, worst compactness: double, worst concavity: double, worst concave points: double, worst symmetry: double, worst fractal dimension: double]"
            ]
          },
          "metadata": {},
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The **schema()** method returns a StructType object:"
      ],
      "metadata": {
        "id": "td0zbQsp3SH_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df.schema"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Flo2sjq_2vlb",
        "outputId": "bdc16670-38dd-456d-a748-ff878bae901f"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "StructType([StructField('mean radius', DoubleType(), True), StructField('mean texture', DoubleType(), True), StructField('mean perimeter', DoubleType(), True), StructField('mean area', DoubleType(), True), StructField('mean smoothness', DoubleType(), True), StructField('mean compactness', DoubleType(), True), StructField('mean concavity', DoubleType(), True), StructField('mean concave points', DoubleType(), True), StructField('mean symmetry', DoubleType(), True), StructField('mean fractal dimension', DoubleType(), True), StructField('radius error', DoubleType(), True), StructField('texture error', DoubleType(), True), StructField('perimeter error', DoubleType(), True), StructField('area error', DoubleType(), True), StructField('smoothness error', DoubleType(), True), StructField('compactness error', DoubleType(), True), StructField('concavity error', DoubleType(), True), StructField('concave points error', DoubleType(), True), StructField('symmetry error', DoubleType(), True), StructField('fractal dimension error', DoubleType(), True), StructField('worst radius', DoubleType(), True), StructField('worst texture', DoubleType(), True), StructField('worst perimeter', DoubleType(), True), StructField('worst area', DoubleType(), True), StructField('worst smoothness', DoubleType(), True), StructField('worst compactness', DoubleType(), True), StructField('worst concavity', DoubleType(), True), StructField('worst concave points', DoubleType(), True), StructField('worst symmetry', DoubleType(), True), StructField('worst fractal dimension', DoubleType(), True)])"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**StructField**\n",
        "\n",
        "`StructFields` model each column in a DataFrame.\n",
        "\n",
        "`StructField` objects are created with the name, dataType, and nullable properties. "
      ],
      "metadata": {
        "id": "wpzq9V1X3rJV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**DoubleType()** can count double in Python as float values are specified with a decimal point. All platforms represent Python float values as 64-bit “double-precision” values"
      ],
      "metadata": {
        "id": "TSqS3O-w5Z5l"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "When the **nullable** field is set to `true`, the column can accept null values."
      ],
      "metadata": {
        "id": "3KFwehF74K9p"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "For convenience, given that the dataset is small, we first construct a Pandas dataframe, tune the schema, and then convert it into a Spark dataframe."
      ],
      "metadata": {
        "id": "uOstTldX-Kk5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def set_df_columns_nullable(spark, df, column_list, nullable=False):\n",
        "    for struct_field in df.schema:\n",
        "        if struct_field.name in column_list:\n",
        "            struct_field.nullable = nullable\n",
        "    df_mod = spark.createDataFrame(df.rdd, df.schema) # creating data frame of distributed data and schema\n",
        "    return df_mod"
      ],
      "metadata": {
        "id": "Er5f3z687le1"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df = set_df_columns_nullable(spark, df, df.columns) # calling defined function"
      ],
      "metadata": {
        "id": "F7dNwPyN80mm"
      },
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "roMrIHua9eqy",
        "outputId": "5baafe03-8530-41b7-b29c-01d864797af0"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "DataFrame[mean radius: double, mean texture: double, mean perimeter: double, mean area: double, mean smoothness: double, mean compactness: double, mean concavity: double, mean concave points: double, mean symmetry: double, mean fractal dimension: double, radius error: double, texture error: double, perimeter error: double, area error: double, smoothness error: double, compactness error: double, concavity error: double, concave points error: double, symmetry error: double, fractal dimension error: double, worst radius: double, worst texture: double, worst perimeter: double, worst area: double, worst smoothness: double, worst compactness: double, worst concavity: double, worst concave points: double, worst symmetry: double, worst fractal dimension: double]"
            ]
          },
          "metadata": {},
          "execution_count": 34
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df = df.withColumn('features', array(df.columns)) # this snippet updates the value of columns as an array to features\n",
        "df"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QsnPpxKK9dy6",
        "outputId": "677a59a0-0c07-4327-9a19-3c7b54d45135"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "DataFrame[mean radius: double, mean texture: double, mean perimeter: double, mean area: double, mean smoothness: double, mean compactness: double, mean concavity: double, mean concave points: double, mean symmetry: double, mean fractal dimension: double, radius error: double, texture error: double, perimeter error: double, area error: double, smoothness error: double, compactness error: double, concavity error: double, concave points error: double, symmetry error: double, fractal dimension error: double, worst radius: double, worst texture: double, worst perimeter: double, worst area: double, worst smoothness: double, worst compactness: double, worst concavity: double, worst concave points: double, worst symmetry: double, worst fractal dimension: double, features: array<double>]"
            ]
          },
          "metadata": {},
          "execution_count": 35
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "vectors = df.rdd.map(lambda row: Vectors.dense(row.features)) # map() transformation on every element of PySpark RDD and learned it returns the same number of elements as input RDD.\n",
        "print(vectors)\n",
        "df.printSchema() # printSchema() method to illustrate that 'features' is a 'struct' column:"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qwXc7ZWO9rje",
        "outputId": "5b831011-a2e1-4088-a509-f5579f54ca26"
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "PythonRDD[119] at RDD at PythonRDD.scala:53\n",
            "root\n",
            " |-- mean radius: double (nullable = false)\n",
            " |-- mean texture: double (nullable = false)\n",
            " |-- mean perimeter: double (nullable = false)\n",
            " |-- mean area: double (nullable = false)\n",
            " |-- mean smoothness: double (nullable = false)\n",
            " |-- mean compactness: double (nullable = false)\n",
            " |-- mean concavity: double (nullable = false)\n",
            " |-- mean concave points: double (nullable = false)\n",
            " |-- mean symmetry: double (nullable = false)\n",
            " |-- mean fractal dimension: double (nullable = false)\n",
            " |-- radius error: double (nullable = false)\n",
            " |-- texture error: double (nullable = false)\n",
            " |-- perimeter error: double (nullable = false)\n",
            " |-- area error: double (nullable = false)\n",
            " |-- smoothness error: double (nullable = false)\n",
            " |-- compactness error: double (nullable = false)\n",
            " |-- concavity error: double (nullable = false)\n",
            " |-- concave points error: double (nullable = false)\n",
            " |-- symmetry error: double (nullable = false)\n",
            " |-- fractal dimension error: double (nullable = false)\n",
            " |-- worst radius: double (nullable = false)\n",
            " |-- worst texture: double (nullable = false)\n",
            " |-- worst perimeter: double (nullable = false)\n",
            " |-- worst area: double (nullable = false)\n",
            " |-- worst smoothness: double (nullable = false)\n",
            " |-- worst compactness: double (nullable = false)\n",
            " |-- worst concavity: double (nullable = false)\n",
            " |-- worst concave points: double (nullable = false)\n",
            " |-- worst symmetry: double (nullable = false)\n",
            " |-- worst fractal dimension: double (nullable = false)\n",
            " |-- features: array (nullable = false)\n",
            " |    |-- element: double (containsNull = false)\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "With the next cell, we build the two datastructures that we will be using throughout this Colab:\n",
        "\n",
        "* `features`, a dataframe of Dense vectors, containing all the original features in the dataset;\n",
        "\n",
        "* `labels`, a series of binary labels indicating if the corresponding set of features belongs to a subject with breast cancer, or not."
      ],
      "metadata": {
        "id": "Z5cJ7xeDCDMj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "class **pyspark.ml.linalg.Vectors**\n",
        "\n",
        "   * Factory methods for working with vectors."
      ],
      "metadata": {
        "id": "9E2y79J5_a67"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.ml.linalg import Vectors\n",
        "features = spark.createDataFrame(vectors.map(Row), [\"features\"])\n",
        "labels = pd.Series(breast_cancer.target)"
      ],
      "metadata": {
        "id": "J1XgQgwnAMbg"
      },
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Your task\n",
        "\n",
        "If you run successfully the Setup and Data Preprocessing stages, you are now ready to cluster the data with the K-means algorithm included in MLlib (Spark's Machine Learning library). Set the `k` parameter to **2**, use a seed of **1**, fit the model, and the compute the Silhouette score (i.e. a measure of quality of the obtained clustering).\n",
        "\n",
        "**IMPORTANT:** use the MLlib implementation of the Silhouette score (via `ClusteringEvaluator`)."
      ],
      "metadata": {
        "id": "DqrmNU_9CiVR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.ml.clustering import KMeans\n",
        "from pyspark.ml.evaluation import ClusteringEvaluator\n",
        "\n",
        "# Trains a k-means model.\n",
        "kmeans = KMeans().setK(2).setSeed(1)\n",
        "model = kmeans.fit(features)\n",
        "\n",
        "# Make predictions\n",
        "predictions = model.transform(features)\n",
        "\n",
        "#Evaluate clustering by computing Silhouette score\n",
        "evaluator = ClusteringEvaluator()\n",
        "\n",
        "silhouette = evaluator.evaluate(predictions)\n",
        "print(\"Silhouette with squared euclidean distance = \" + str(silhouette))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kR5zy3Y7CB9o",
        "outputId": "2e8d045e-c165-410b-bd05-6d7e7fbf5a80"
      },
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Silhouette with squared euclidean distance = 0.8342904262826145\n"
          ]
        }
      ]
    }
  ]
}