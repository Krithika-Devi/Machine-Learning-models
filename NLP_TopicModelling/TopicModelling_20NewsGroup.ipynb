{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "> ## KRITHIKA DEVI CHANDRAN (2211570)\n",
        "> ## Domain: AI & ML\n",
        "\n",
        "> # Natural Language Processing"
      ],
      "metadata": {
        "id": "4b7-0wMhmEoC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Topic Modeling\n",
        "\n",
        "This Project takes a few news group data and identifies the topic of discussion in each of these groups.\n",
        "\n",
        "Here we are taking threads from 20 news groups as to reduce processing time."
      ],
      "metadata": {
        "id": "mQ0PwfflbEKS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Dataset Description\n",
        "\n",
        "**Data: The 20 newsgroups text dataset**\n",
        "\n",
        "The dataset comprises around **18000 newsgroups posts** on **20 topics** split in two subsets: one for training (or development) and the other one for testing (or for performance evaluation). The split between the train and test set is based upon a messages posted before and after a specific date."
      ],
      "metadata": {
        "id": "slfaq25HdE09"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The `sklearn.datasets.fetch_20newsgroups` function is a data fetching / caching functions that downloads the data archive from the original 20 newsgroups website, extracts the archive contents in the `~/scikit_learn_data/20news_home` folder and calls the `sklearn.datasets.load_files` on either the training or testing set folder, or both of them:"
      ],
      "metadata": {
        "id": "0ChN4Bq2Egal"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "vvSRtSXka3Rg"
      },
      "outputs": [],
      "source": [
        "# importing 20 newsgroups dataset using sklearn\n",
        "from sklearn.datasets import fetch_20newsgroups"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ignoring warnings\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')"
      ],
      "metadata": {
        "id": "pp2Pr3Q4bTZM"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# getting train and test set data\n",
        "newsgroups_train = fetch_20newsgroups(subset='train',shuffle = True)\n",
        "newsgroups_test = fetch_20newsgroups(subset='test',shuffle = True)"
      ],
      "metadata": {
        "id": "gAaRiYB4bU-6"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# getting names of the document (target_names) \n",
        "list(newsgroups_train.target_names)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "x8jeXE3RbWPg",
        "outputId": "d4f1cf17-0eee-4a93-a594-10a3249a04c2"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['alt.atheism',\n",
              " 'comp.graphics',\n",
              " 'comp.os.ms-windows.misc',\n",
              " 'comp.sys.ibm.pc.hardware',\n",
              " 'comp.sys.mac.hardware',\n",
              " 'comp.windows.x',\n",
              " 'misc.forsale',\n",
              " 'rec.autos',\n",
              " 'rec.motorcycles',\n",
              " 'rec.sport.baseball',\n",
              " 'rec.sport.hockey',\n",
              " 'sci.crypt',\n",
              " 'sci.electronics',\n",
              " 'sci.med',\n",
              " 'sci.space',\n",
              " 'soc.religion.christian',\n",
              " 'talk.politics.guns',\n",
              " 'talk.politics.mideast',\n",
              " 'talk.politics.misc',\n",
              " 'talk.religion.misc']"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The real data lies in the `filenames` and `target` attributes. The target attribute is the integer index of the category:"
      ],
      "metadata": {
        "id": "ZF3tXtJwJSoJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Shape of filename: \", newsgroups_train.filenames.shape)\n",
        "print(\"Shape of target: \", newsgroups_train.target.shape)\n",
        "newsgroups_train.target[:10]"
      ],
      "metadata": {
        "id": "ZABNXCJnIcOp",
        "outputId": "8d82611c-4ff5-4d70-d495-7bb08db4da45",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Shape of filename:  (11314,)\n",
            "Shape of target:  (11314,)\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([ 7,  4,  4,  1, 14, 16, 13,  3,  2,  4])"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# fetching first two filenames in data\n",
        "newsgroups_train.data[:2]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZWPSzvYAbXkF",
        "outputId": "86f0d06c-5255-49ea-a582-f9088a14f02f"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[\"From: lerxst@wam.umd.edu (where's my thing)\\nSubject: WHAT car is this!?\\nNntp-Posting-Host: rac3.wam.umd.edu\\nOrganization: University of Maryland, College Park\\nLines: 15\\n\\n I was wondering if anyone out there could enlighten me on this car I saw\\nthe other day. It was a 2-door sports car, looked to be from the late 60s/\\nearly 70s. It was called a Bricklin. The doors were really small. In addition,\\nthe front bumper was separate from the rest of the body. This is \\nall I know. If anyone can tellme a model name, engine specs, years\\nof production, where this car is made, history, or whatever info you\\nhave on this funky looking car, please e-mail.\\n\\nThanks,\\n- IL\\n   ---- brought to you by your neighborhood Lerxst ----\\n\\n\\n\\n\\n\",\n",
              " \"From: guykuo@carson.u.washington.edu (Guy Kuo)\\nSubject: SI Clock Poll - Final Call\\nSummary: Final call for SI clock reports\\nKeywords: SI,acceleration,clock,upgrade\\nArticle-I.D.: shelley.1qvfo9INNc3s\\nOrganization: University of Washington\\nLines: 11\\nNNTP-Posting-Host: carson.u.washington.edu\\n\\nA fair number of brave souls who upgraded their SI clock oscillator have\\nshared their experiences for this poll. Please send a brief message detailing\\nyour experiences with the procedure. Top speed attained, CPU rated speed,\\nadd on cards and adapters, heat sinks, hour of usage per day, floppy disk\\nfunctionality with 800 and 1.4 m floppies are especially requested.\\n\\nI will be summarizing in the next two days, so please add to the network\\nknowledge base if you have done the clock upgrade and haven't answered this\\npoll. Thanks.\\n\\nGuy Kuo <guykuo@u.washington.edu>\\n\"]"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Importing Libraries\n"
      ],
      "metadata": {
        "id": "yOlv1_MTMUIm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "* **Gensim library** \n",
        "\n",
        "   Gensim is designed to handle large text collections using data streaming and incremental online algorithms, which differentiates it from most other machine learning software packages that target only in-memory processing.\n",
        "\n",
        "* **Preprocessing library**\n",
        "   \n",
        "   Importing `simple_preprocess` from `gensim.utils` package, which converts a document into a list of tokens. This lowercases, tokenizes, de-accents (optional). – the output are final tokens = unicode strings, that won’t be processed any further.\n",
        "\n",
        "* **Preprocessing - Parsing library**\n",
        "\n",
        "  Parsing is the process of analyzing a string of words to the rules of grammar which breaks up into component parts. This determines the syntactic structure of an expression. A module `gensim.parsing.preprocessing` contains methods for parsing and preprocessing strings. From this imported `STOPWORDS`."
      ],
      "metadata": {
        "id": "i0nKrP0LMGeT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import gensim\n",
        "from gensim.utils import simple_preprocess\n",
        "from gensim.parsing.preprocessing import STOPWORDS"
      ],
      "metadata": {
        "id": "NE-pa5GYbZaX"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Importing `nltk` (natural language toolkit) library and `wordnet` is downloaded which is a lexical database for the English language which is a part of the NLTK corpus."
      ],
      "metadata": {
        "id": "9JWOb9GgVIrg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('wordnet')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7Gb4IbAcbbGN",
        "outputId": "2066ded4-6690-4996-ceba-fa23d3221e1e"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "* **Lemmatization and Stemming**\n",
        "\n",
        "    * **Lemmatization** is the process of grouping together the different inflected forms of a word so they can be analyzed as a single item. It links words with similar meanings to one word.\n",
        "\n",
        "    * **Stemming** is the process of producing morphological variants of a root/base word. It removes the last few words or suffix of a word where it misspelt or incorrect words.\n",
        "\n",
        "    Two libraries `WordNetLemmatizer` and `SnowballStemmer` is imported:\n",
        "\n",
        "    1. `WordNetLemmatizer` is a built-in morphy function and returns the input word unchanged if it cannot be found in WordNet.\n",
        "    2. `SnowballStemmer` can map non-English words too.\n",
        "\n",
        "\n",
        "* **Porter Stemmer** is used which suffixes in the English language are made up of a combination of smaller and simpler suffixes.\n",
        "\n",
        "    Every module `*` is imported from `nltk.stem.porter`. \n"
      ],
      "metadata": {
        "id": "eBcyCbFHWBhf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.stem import WordNetLemmatizer, SnowballStemmer\n",
        "from nltk.stem.porter import *\n",
        "import numpy as np # importing numpy library\n",
        "np.random.seed(400) # setting random seed"
      ],
      "metadata": {
        "id": "bD0FR-RUbd2q"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Downloading `omw-1.4` from `nltk` library, which is a **Open Multilingual Wordnet (OMW)**, shares a  common function, but produce output of differing types comparing to Wornet."
      ],
      "metadata": {
        "id": "6EDTXRnqclys"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('omw-1.4')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hiNxKWeObgqF",
        "outputId": "52d98dca-278a-47ec-e014-374b6671f123"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package omw-1.4 to /root/nltk_data...\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "In order to lemmatize, you need to create an instance of the `WordNetLemmatizer()` and call the `lemmatize()` function on a single word. \n",
        "\n",
        "Here called a word, `went` to lemmatize into `pos = 'v'`, a parts of speech as verb."
      ],
      "metadata": {
        "id": "m6rPSIUhXzH_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(WordNetLemmatizer().lemmatize('went', pos = 'v'))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ue00qvLUbia8",
        "outputId": "ff9bda07-bd0a-4f51-b284-ec8ceae0e662"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "go\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd # pandas library\n",
        "stemmer = SnowballStemmer(\"english\") # getting a word `english` for stemming"
      ],
      "metadata": {
        "id": "Mg1f6sirbj_s"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# reading many words for stemming\n",
        "original_words = ['caresses', 'flies', 'dies', 'mules', 'denied','died', 'agreed', 'owned',\n",
        "           'humbled', 'sized','meeting', 'stating', 'siezing', 'itemization','sensational',\n",
        "           'traditional', 'reference', 'colonizer','plotted']\n",
        "singles = [stemmer.stem(plural) for plural in original_words]"
      ],
      "metadata": {
        "id": "lzxb8QjibmX7"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pd.DataFrame(data={'original word':original_words, 'stemmed':singles })"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 645
        },
        "id": "YAHDcVUubn8-",
        "outputId": "06a541a7-d39c-4710-95a9-286540480cda"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "   original word stemmed\n",
              "0       caresses  caress\n",
              "1          flies     fli\n",
              "2           dies     die\n",
              "3          mules    mule\n",
              "4         denied    deni\n",
              "5           died     die\n",
              "6         agreed    agre\n",
              "7          owned     own\n",
              "8        humbled   humbl\n",
              "9          sized    size\n",
              "10       meeting    meet\n",
              "11       stating   state\n",
              "12       siezing    siez\n",
              "13   itemization    item\n",
              "14   sensational  sensat\n",
              "15   traditional  tradit\n",
              "16     reference   refer\n",
              "17     colonizer   colon\n",
              "18       plotted    plot"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-4192f4ba-2f8a-4b5c-8512-e08eba812768\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>original word</th>\n",
              "      <th>stemmed</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>caresses</td>\n",
              "      <td>caress</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>flies</td>\n",
              "      <td>fli</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>dies</td>\n",
              "      <td>die</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>mules</td>\n",
              "      <td>mule</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>denied</td>\n",
              "      <td>deni</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>died</td>\n",
              "      <td>die</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>agreed</td>\n",
              "      <td>agre</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>owned</td>\n",
              "      <td>own</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>humbled</td>\n",
              "      <td>humbl</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>sized</td>\n",
              "      <td>size</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10</th>\n",
              "      <td>meeting</td>\n",
              "      <td>meet</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11</th>\n",
              "      <td>stating</td>\n",
              "      <td>state</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12</th>\n",
              "      <td>siezing</td>\n",
              "      <td>siez</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13</th>\n",
              "      <td>itemization</td>\n",
              "      <td>item</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14</th>\n",
              "      <td>sensational</td>\n",
              "      <td>sensat</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15</th>\n",
              "      <td>traditional</td>\n",
              "      <td>tradit</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>16</th>\n",
              "      <td>reference</td>\n",
              "      <td>refer</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>17</th>\n",
              "      <td>colonizer</td>\n",
              "      <td>colon</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>18</th>\n",
              "      <td>plotted</td>\n",
              "      <td>plot</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-4192f4ba-2f8a-4b5c-8512-e08eba812768')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-4192f4ba-2f8a-4b5c-8512-e08eba812768 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-4192f4ba-2f8a-4b5c-8512-e08eba812768');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Definition function for Wordnet lemmatization:"
      ],
      "metadata": {
        "id": "C7Bjzrt6dlQ8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def lemmatize_stemming(text):\n",
        "    return stemmer.stem(WordNetLemmatizer().lemmatize(text, pos='v'))"
      ],
      "metadata": {
        "id": "3cx1bXgubpSj"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Definition function for preprocessing the parseing, of stopwords:\n",
        "\n"
      ],
      "metadata": {
        "id": "c1rNvp2OdwIG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Tokenize and lemmatize\n",
        "def preprocess(text):\n",
        "    result=[]\n",
        "    for token in gensim.utils.simple_preprocess(text) :\n",
        "        if token not in gensim.parsing.preprocessing.STOPWORDS and len(token) > 3:\n",
        "            result.append(lemmatize_stemming(token))\n",
        "\n",
        "    return result"
      ],
      "metadata": {
        "id": "PoYe7SnUbrFy"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# setting document number 50\n",
        "document_num = 50\n",
        "# writing a sample word for documenting the sample\n",
        "doc_sample = 'This disk has failed many times. I would like to get it replaced.'"
      ],
      "metadata": {
        "id": "RJryhiQZbsU-"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Original document: \")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xA_JpxWVbu_X",
        "outputId": "8b12f37c-7806-4a4c-cb0c-c54de00c1b3b"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original document: \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Splitting a string from the sample word."
      ],
      "metadata": {
        "id": "PCUET_eoehVo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "words = []\n",
        "for word in doc_sample.split(' '):\n",
        "  words.append(word)\n",
        "\n",
        "print(words)  "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iJ6m9Y84bwMs",
        "outputId": "0b77feac-350d-421c-bd07-649963d63063"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['This', 'disk', 'has', 'failed', 'many', 'times.', 'I', 'would', 'like', 'to', 'get', 'it', 'replaced.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Printing the tokenized and lemmatized of sample sentence:\n"
      ],
      "metadata": {
        "id": "l6B0xL7de5NF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"\\n\\nTokenized and lemmatized document: \")\n",
        "print(preprocess(doc_sample))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kbRrwzGPbxLj",
        "outputId": "399045e6-65c6-4280-a47d-0f66e4d58304"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "Tokenized and lemmatized document: \n",
            "['disk', 'fail', 'time', 'like', 'replac']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Preprocessing the 20 news groups data that performs lowercases, tokenization, de-accent solution with in-built."
      ],
      "metadata": {
        "id": "UPRF3j75fGK-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "processed_docs = []\n",
        " \n",
        "for doc in newsgroups_train.data:\n",
        "    processed_docs.append(preprocess(doc))\n",
        " \n",
        "print(processed_docs[:2])\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PH69wBsVbyr0",
        "outputId": "2a6bd4c0-af9e-4449-fc5c-2d62aa40a9dd"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[['lerxst', 'thing', 'subject', 'nntp', 'post', 'host', 'organ', 'univers', 'maryland', 'colleg', 'park', 'line', 'wonder', 'enlighten', 'door', 'sport', 'look', 'late', 'earli', 'call', 'bricklin', 'door', 'small', 'addit', 'bumper', 'separ', 'rest', 'bodi', 'know', 'tellm', 'model', 'engin', 'spec', 'year', 'product', 'histori', 'info', 'funki', 'look', 'mail', 'thank', 'bring', 'neighborhood', 'lerxst'], ['guykuo', 'carson', 'washington', 'subject', 'clock', 'poll', 'final', 'summari', 'final', 'clock', 'report', 'keyword', 'acceler', 'clock', 'upgrad', 'articl', 'shelley', 'qvfo', 'innc', 'organ', 'univers', 'washington', 'line', 'nntp', 'post', 'host', 'carson', 'washington', 'fair', 'number', 'brave', 'soul', 'upgrad', 'clock', 'oscil', 'share', 'experi', 'poll', 'send', 'brief', 'messag', 'detail', 'experi', 'procedur', 'speed', 'attain', 'rat', 'speed', 'card', 'adapt', 'heat', 'sink', 'hour', 'usag', 'floppi', 'disk', 'function', 'floppi', 'especi', 'request', 'summar', 'day', 'network', 'knowledg', 'base', 'clock', 'upgrad', 'haven', 'answer', 'poll', 'thank', 'guykuo', 'washington']]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "`corpora.Dictionary` is imported from `gensim` which construct word<->id mappings. This module implements the concept of a Dictionary – a mapping between words and their integer ids."
      ],
      "metadata": {
        "id": "ukKokiWJf7Ma"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dictionary = gensim.corpora.Dictionary(processed_docs)"
      ],
      "metadata": {
        "id": "lWTf08C6b0Fh"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# counting every word in a dictionary and eliminating if any count of words is greater than 100\n",
        "count = 0\n",
        "for k, v in dictionary.iteritems():\n",
        "    print(k, v)\n",
        "    count += 1\n",
        "    if count > 100:\n",
        "        break"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8aCUpLJub2Yi",
        "outputId": "06297374-aeeb-4a86-9ab7-7f1ca07b6c3e"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0 addit\n",
            "1 bodi\n",
            "2 bricklin\n",
            "3 bring\n",
            "4 bumper\n",
            "5 call\n",
            "6 colleg\n",
            "7 door\n",
            "8 earli\n",
            "9 engin\n",
            "10 enlighten\n",
            "11 funki\n",
            "12 histori\n",
            "13 host\n",
            "14 info\n",
            "15 know\n",
            "16 late\n",
            "17 lerxst\n",
            "18 line\n",
            "19 look\n",
            "20 mail\n",
            "21 maryland\n",
            "22 model\n",
            "23 neighborhood\n",
            "24 nntp\n",
            "25 organ\n",
            "26 park\n",
            "27 post\n",
            "28 product\n",
            "29 rest\n",
            "30 separ\n",
            "31 small\n",
            "32 spec\n",
            "33 sport\n",
            "34 subject\n",
            "35 tellm\n",
            "36 thank\n",
            "37 thing\n",
            "38 univers\n",
            "39 wonder\n",
            "40 year\n",
            "41 acceler\n",
            "42 adapt\n",
            "43 answer\n",
            "44 articl\n",
            "45 attain\n",
            "46 base\n",
            "47 brave\n",
            "48 brief\n",
            "49 card\n",
            "50 carson\n",
            "51 clock\n",
            "52 day\n",
            "53 detail\n",
            "54 disk\n",
            "55 especi\n",
            "56 experi\n",
            "57 fair\n",
            "58 final\n",
            "59 floppi\n",
            "60 function\n",
            "61 guykuo\n",
            "62 haven\n",
            "63 heat\n",
            "64 hour\n",
            "65 innc\n",
            "66 keyword\n",
            "67 knowledg\n",
            "68 messag\n",
            "69 network\n",
            "70 number\n",
            "71 oscil\n",
            "72 poll\n",
            "73 procedur\n",
            "74 qvfo\n",
            "75 rat\n",
            "76 report\n",
            "77 request\n",
            "78 send\n",
            "79 share\n",
            "80 shelley\n",
            "81 sink\n",
            "82 soul\n",
            "83 speed\n",
            "84 summar\n",
            "85 summari\n",
            "86 upgrad\n",
            "87 usag\n",
            "88 washington\n",
            "89 access\n",
            "90 activ\n",
            "91 actual\n",
            "92 advanc\n",
            "93 anybodi\n",
            "94 anymor\n",
            "95 appear\n",
            "96 better\n",
            "97 breifli\n",
            "98 bunch\n",
            "99 convict\n",
            "100 corner\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "`dictionary.filter_extremes`, filter out tokens that appear in \n",
        "1. less than no_below documents (absolute number) or\n",
        "2. more than no_above documents (fraction of total corpus size, not absolute number).\n",
        "3. after (1) and (2), keep only the first keep_n most frequent tokens (or keep all if None).\n",
        "\n",
        "After the pruning, shrink resulting gaps in word ids.\n",
        "\n"
      ],
      "metadata": {
        "id": "AW4kHZuWgZlw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dictionary.filter_extremes(no_below = 15, no_above = 0.1, keep_n = 10000)"
      ],
      "metadata": {
        "id": "-Dev162Sb8KW"
      },
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "`dictionary.doc2bow` converts document (a list of words) into the bag-of-words format = list of (token_id, token_count) 2-tuples. Each word is assumed to be a **tokenized and normalized** string (either unicode or utf8-encoded). No further preprocessing is done on the words in document; apply tokenization, stemming etc. before calling this method."
      ],
      "metadata": {
        "id": "FybMy99fhPkM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In Gensim, the corpus contains the word id and its frequency in every document. We can create a `BoW corpus` from a simple list of documents and from text files."
      ],
      "metadata": {
        "id": "pqslOjKeh8DT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "bow_corpus = [dictionary.doc2bow(doc) for doc in processed_docs]\n",
        " \n",
        "document_num = 20 # assigning a document number 20, since there is a 20 News Groups data\n",
        "bow_doc_x = bow_corpus[document_num] # to contain word id and its frequency in every document\n",
        " "
      ],
      "metadata": {
        "id": "Qht7Vv_Eb-H1"
      },
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "bow_doc_x"
      ],
      "metadata": {
        "id": "3MbFLU2viQcv",
        "outputId": "fc845b05-b446-4fcd-a6b3-59a6e641ae2f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[(18, 1),\n",
              " (166, 1),\n",
              " (336, 1),\n",
              " (350, 1),\n",
              " (391, 1),\n",
              " (437, 1),\n",
              " (453, 1),\n",
              " (476, 1),\n",
              " (480, 1),\n",
              " (482, 3),\n",
              " (520, 2),\n",
              " (721, 3),\n",
              " (732, 1),\n",
              " (803, 1),\n",
              " (859, 1),\n",
              " (917, 1),\n",
              " (990, 1),\n",
              " (991, 1),\n",
              " (992, 1),\n",
              " (993, 1),\n",
              " (994, 2),\n",
              " (995, 1),\n",
              " (996, 1),\n",
              " (997, 3),\n",
              " (998, 1),\n",
              " (999, 2),\n",
              " (1000, 2),\n",
              " (1001, 1),\n",
              " (1002, 1),\n",
              " (1003, 1),\n",
              " (1004, 1),\n",
              " (1005, 1),\n",
              " (1006, 1),\n",
              " (1007, 1),\n",
              " (1008, 1),\n",
              " (1009, 1)]"
            ]
          },
          "metadata": {},
          "execution_count": 38
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# printing particular word with their counts along with how much time they appear.\n",
        "for i in range(len(bow_doc_x)):\n",
        "    print(\"Word {} (\\\"{}\\\") appears {} time.\".format(bow_doc_x[i][0], \n",
        "                                                     dictionary[bow_doc_x[i][0]], \n",
        "                                                     bow_doc_x[i][1]))"
      ],
      "metadata": {
        "id": "w-U__ywuiTWt",
        "outputId": "ef352dd6-27cf-4e1c-c326-9bf315f0b540",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Word 18 (\"rest\") appears 1 time.\n",
            "Word 166 (\"clear\") appears 1 time.\n",
            "Word 336 (\"refer\") appears 1 time.\n",
            "Word 350 (\"true\") appears 1 time.\n",
            "Word 391 (\"technolog\") appears 1 time.\n",
            "Word 437 (\"christian\") appears 1 time.\n",
            "Word 453 (\"exampl\") appears 1 time.\n",
            "Word 476 (\"jew\") appears 1 time.\n",
            "Word 480 (\"lead\") appears 1 time.\n",
            "Word 482 (\"littl\") appears 3 time.\n",
            "Word 520 (\"wors\") appears 2 time.\n",
            "Word 721 (\"keith\") appears 3 time.\n",
            "Word 732 (\"punish\") appears 1 time.\n",
            "Word 803 (\"california\") appears 1 time.\n",
            "Word 859 (\"institut\") appears 1 time.\n",
            "Word 917 (\"similar\") appears 1 time.\n",
            "Word 990 (\"allan\") appears 1 time.\n",
            "Word 991 (\"anti\") appears 1 time.\n",
            "Word 992 (\"arriv\") appears 1 time.\n",
            "Word 993 (\"austria\") appears 1 time.\n",
            "Word 994 (\"caltech\") appears 2 time.\n",
            "Word 995 (\"distinguish\") appears 1 time.\n",
            "Word 996 (\"german\") appears 1 time.\n",
            "Word 997 (\"germani\") appears 3 time.\n",
            "Word 998 (\"hitler\") appears 1 time.\n",
            "Word 999 (\"livesey\") appears 2 time.\n",
            "Word 1000 (\"motto\") appears 2 time.\n",
            "Word 1001 (\"order\") appears 1 time.\n",
            "Word 1002 (\"pasadena\") appears 1 time.\n",
            "Word 1003 (\"pompous\") appears 1 time.\n",
            "Word 1004 (\"popul\") appears 1 time.\n",
            "Word 1005 (\"rank\") appears 1 time.\n",
            "Word 1006 (\"schneider\") appears 1 time.\n",
            "Word 1007 (\"semit\") appears 1 time.\n",
            "Word 1008 (\"social\") appears 1 time.\n",
            "Word 1009 (\"solntz\") appears 1 time.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "`gensims.models.ldamulticore` - is an online **Latent Dirichlet Allocation (LDA)** in Python, using all CPU cores to parallelize and speed up model training. This parallelization uses multiprocessing."
      ],
      "metadata": {
        "id": "9rp-xbWEjQFr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "lda_model =  gensim.models.LdaMulticore(bow_corpus, \n",
        "                                   num_topics = 8,\n",
        "                                   id2word = dictionary,                                   \n",
        "                                   passes = 10,\n",
        "                                   workers = 2)"
      ],
      "metadata": {
        "id": "6BOhh5fLb_Wu"
      },
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Topic Modeling\n",
        "\n",
        "Topic Modeling is a technique to extract the hidden topics from large volumes of text. Latent Dirichlet Allocation(LDA) is a popular algorithm for topic modeling with excellent implementations in the Python’s Gensim package. The challenge, however, is **how to extract good quality of topics that are clear, segregated and meaningful**. This depends heavily on the quality of text preprocessing and the strategy of finding the optimal number of topics. \n",
        "\n",
        "To see the keywords for each topic and the weightage(importance) of each keyword using `lda_model.print_topics()`."
      ],
      "metadata": {
        "id": "9e6j4Ar8jpb-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for idx, topic in lda_model.print_topics(-1):\n",
        "    print(\"Topic: {} \\nWords: {}\".format(idx, topic ))\n",
        "    print(\"\\n\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f2XAfNjFcDZr",
        "outputId": "72c9ca40-3800-4aa1-b42e-760dfc938c56"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Topic: 0 \n",
            "Words: 0.012*\"armenian\" + 0.006*\"turkish\" + 0.005*\"bike\" + 0.005*\"leav\" + 0.004*\"road\" + 0.004*\"home\" + 0.004*\"turk\" + 0.003*\"greek\" + 0.003*\"game\" + 0.003*\"armenia\"\n",
            "\n",
            "\n",
            "Topic: 1 \n",
            "Words: 0.021*\"drive\" + 0.007*\"control\" + 0.007*\"disk\" + 0.006*\"hard\" + 0.004*\"caus\" + 0.004*\"pitt\" + 0.004*\"effect\" + 0.003*\"gordon\" + 0.003*\"firearm\" + 0.003*\"price\"\n",
            "\n",
            "\n",
            "Topic: 2 \n",
            "Words: 0.008*\"wire\" + 0.008*\"scsi\" + 0.008*\"power\" + 0.005*\"data\" + 0.004*\"design\" + 0.004*\"grind\" + 0.004*\"circuit\" + 0.004*\"chip\" + 0.004*\"engin\" + 0.004*\"connect\"\n",
            "\n",
            "\n",
            "Topic: 3 \n",
            "Words: 0.012*\"space\" + 0.010*\"nasa\" + 0.006*\"presid\" + 0.005*\"nation\" + 0.005*\"research\" + 0.005*\"program\" + 0.004*\"center\" + 0.004*\"orbit\" + 0.004*\"health\" + 0.004*\"group\"\n",
            "\n",
            "\n",
            "Topic: 4 \n",
            "Words: 0.010*\"govern\" + 0.009*\"encrypt\" + 0.007*\"secur\" + 0.007*\"israel\" + 0.006*\"chip\" + 0.006*\"public\" + 0.006*\"clipper\" + 0.006*\"isra\" + 0.004*\"protect\" + 0.004*\"key\"\n",
            "\n",
            "\n",
            "Topic: 5 \n",
            "Words: 0.018*\"game\" + 0.016*\"team\" + 0.012*\"play\" + 0.010*\"player\" + 0.009*\"hockey\" + 0.008*\"toronto\" + 0.006*\"season\" + 0.006*\"canada\" + 0.005*\"score\" + 0.005*\"leagu\"\n",
            "\n",
            "\n",
            "Topic: 6 \n",
            "Words: 0.016*\"window\" + 0.014*\"file\" + 0.010*\"program\" + 0.007*\"card\" + 0.007*\"version\" + 0.006*\"softwar\" + 0.006*\"imag\" + 0.005*\"graphic\" + 0.005*\"avail\" + 0.005*\"color\"\n",
            "\n",
            "\n",
            "Topic: 7 \n",
            "Words: 0.010*\"christian\" + 0.006*\"jesus\" + 0.006*\"exist\" + 0.004*\"claim\" + 0.004*\"moral\" + 0.004*\"human\" + 0.004*\"jew\" + 0.004*\"word\" + 0.004*\"religion\" + 0.004*\"life\"\n",
            "\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "*Summary:*\n",
        "\n",
        "**Topic: 0** is represented as,\n",
        "\n",
        "Words: 0.012*\"armenian\" + 0.006*\"turkish\" + 0.005*\"bike\" + 0.005*\"leav\" + 0.004*\"road\" + 0.004*\"home\" + 0.004*\"turk\" + 0.003*\"greek\" + 0.003*\"game\" + 0.003*\"armenia\".\n",
        "\n",
        "It means the top 10 keywords that contribute to this topic are: 'armenian', 'turkish', 'bike', ... and so on and the weight of 'armenian' on topic 0 is 0.012.\n",
        "\n",
        "**The weights reflect how important a keyword is to that topic.**\n"
      ],
      "metadata": {
        "id": "PqCOV0WtlP9W"
      }
    }
  ]
}